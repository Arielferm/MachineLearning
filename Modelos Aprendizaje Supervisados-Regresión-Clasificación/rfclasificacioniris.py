# -*- coding: utf-8 -*-
"""RFClasificacionIRIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lTbvTduvmrLJ_8E045rLCK0QLNgVLxSf

# Árboles de decisión

De la misma manera que lo hicimos con las máquinas de soporte vectorial, intentaremos usar los árboles de decisión para clasificar los elementos que pertenecen al dataset Iris.

Como recordamos del ejemplo anterior, esta base de datos toma los datos de la longitud y el ancho de los pétalos para intentar clasificar las diferentes especies de Iris.

Empezaremos importando los módulos y la base de datos que utilizaremos en este ejemplo:
"""

#Comenzamos importando los módulos necesarios:
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn import metrics

# Las estrellas de este notebook
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn import tree

from sklearn import datasets
import pandas as pd

#Importamos el dataset, y lo convertimos en un DataFrame para poder visualizarlo fácilmente:
iris=datasets.load_iris()
X = iris.data
Y = iris.target
X = X[:, 2:]  # Nos quedamos solo con el largo y el ancho del pétalo.

df = pd.DataFrame(X, columns=iris.feature_names[2:])
df['label'] = iris.target
df.head()

"""## Clasificación

De la misma manera que lo hicimos para las máquinas de soporte vectorial, dividimos nuestra base de datos en dos partes: Una de ellas la usaremos para entrenar nuestro árbol de decisión, y la otra para probar las predicciones. Luego creamos un árbol de decisión y lo entrenamos con el primer bloque de entradas y salidas, y graficamos las predicciones de nuestro sistema:
"""

#Divido en datos de entrenamiento y de testeo:
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=1)

# Creo el árbol de decisión y lo entreno con los datos:
tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X_train,y_train)

#Y ahora genero un gráfico con los resultados. en primer lugar, defino los valores máximos y mínimos de x y de y (nuestros valores de entrada y salida)
x_min, x_max = X_test[:, 0].min() - .5, X_test[:, 0].max() + .5
y_min, y_max = X_test[:, 1].min() - .5, X_test[:, 1].max() + .5

#Genero un rango de valores entre el máimo y el minimo, con un paso de 0.02 entre un valor y el siguiente:
h = .02
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

#Genero ahora las predicciones para los valores de x de testeo:
Z = tree_clf.predict(np.c_[xx.ravel(), yy.ravel()])

# Y dibujo los valores:
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Dibujo también todos los puntos que usamos para testear el sistema:
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Petal length')
plt.ylabel('Petal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())

plt.show()

"""Vemos que las regiones son rectangulos. En particular, hay dos rectas de decision: una utiliza el length (depth=0) y la otra el width (depth=1). Si cambiamos el max depth del algoritmo, veriamos aparecer mas rectas correspondientes a decisiones binarias que hace el arbol.

El funcionamiento del árbol de decisión podemos verlo si utilizamos la función **plot_tree** que nos permite ver las decisiones binarias que va tomando el árbol con cada variable:

"""

import cv2

tree.export_graphviz(
tree_clf,
out_file="iris_tree.dot",
feature_names=iris.feature_names[2:],
class_names=iris.target_names,
rounded=True,
filled=True
)

#Convierto el dot a png
!dot -Tpng iris_tree.dot -o iris_tree.png

#Ploteamos el png
img = cv2.imread('iris_tree.png')
plt.figure(figsize = (10, 10))
plt.imshow(img)



"""El algoritmo tiene distintos parametros. En particular, podemos elegir si utiliza Gini o Entropia para calcular la impureza de un splitting. Generalmente no hay diferencia pero por definicion Gini puede favorecer mas la clase mas frecuente. La ventaja es la rapidez de Gini por sobre Entropia.

El árbol que se genera no es el optimo, pero es razonablemente bueno. Esta diferencia se hace mas pronunciada mientras mas profundo sea el arbol. Esto se debe a que la decisión en que cada splitting no tiene en cuenta lo que pase en los splittings subsiguientes. Es lo que se llama un *greedy algorithm*.

Un problema que presentan los árboles de decisión es que, al ser un modelo no parametrico, es propenso a generar overfitting. Por eso es necesario regularizarlo restringiendo la libertad del arbol. Las opciones que tenemos en sklearn son:


*   **max_depth**: por defecto es None, controla la profundidad del arbol.
*    **min_samples_split**: establece el minimo numero de muestras que debe tener un nodo para poder seguir partiendolo.
*    **min_samples_leaf**: el minimo numero de muestras que debe tener una hoja (ie nodo final).
*    **min_weight_fraction_leaf**: la minima fraccion pesada de muestras que debe poseer una hoja.
*    **max_leaf_nodes**: maxima cantidad de hojas.
*    **max_features**: maxima cantidad de features evaluados en un splitting.

Si uno sube los valores minimos o baja los maximos, esta restringiendo al arbol y regularizando el modelo.

Existen otros metodos de regularizacion como por ejemplo el pruning o podado de arboles en el que se entrena sin restricciones y luego se eliminan nodos innecesarios.

Veamos por ejemplo lo que pasa si ejecutamos el mismo algoritmo que hicimos anteriormente pero quitando la limitación de **max_depth=2**:
"""

# Vuelvo a crear el arbol y lo ajusto a los datos nuevamente:
tree_clf = DecisionTreeClassifier()
tree_clf.fit(X_train, y_train)

#Y vuelvo a realizar el gráfico:

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, m_max]x[y_min, y_max].
x_min, x_max = X_test[:, 0].min() - .5, X_test[:, 0].max() + .5
y_min, y_max = X_test[:, 1].min() - .5, X_test[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = tree_clf.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Petal length')
plt.ylabel('Petal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())

plt.show()

"""Vemos como el algoritmo generó un overfitting para poder intentar contener todos los puntos, inclusive aquellos que generan mayor conflicto en la clasificación.

Veamos como quedó el árbol, y podemos compararlo con aquel cuyo max_depth=2:
"""

import cv2

tree.export_graphviz(
tree_clf,
out_file="iris_tree.dot",
feature_names=iris.feature_names[2:],
class_names=iris.target_names,
rounded=True,
filled=True
)

#Convierto el dot a png
!dot -Tpng iris_tree.dot -o iris_tree.png

#Ploteamos el png
img = cv2.imread('iris_tree.png')
plt.figure(figsize = (10, 10))
plt.imshow(img)

"""Y por último veamos la efectividad que presenta el árbol si no lo restringimos únicamente a los valores del ancho y largo del pétalo sino que tomamos las 4 posibles entradas de la base de datos:"""

# store the feature matrix (X) and response vector (y)
X = iris.data
y = iris.target

# splitting X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)

# training the model on training set
tree_clf = DecisionTreeClassifier(max_depth=7)
tree_clf.fit(X_train, y_train)

# making predictions on the testing set
y_pred = tree_clf.predict(X_test)

# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print("Decission Tree model accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)

"""Vemos que posee una efectividad del 96,67%, lo que representa un alto grado de efectividad para la clasificación en esta aplicación. Veamos también el árbol que se generó en este caso:"""

import cv2

tree.export_graphviz(
tree_clf,
out_file="iris_tree.dot",
feature_names=iris.feature_names,
class_names=iris.target_names,
rounded=True,
filled=True
)

#Convierto el dot a png
!dot -Tpng iris_tree.dot -o iris_tree.png

#Ploteamos el png
img = cv2.imread('iris_tree.png')
plt.figure(figsize = (10, 10))
plt.imshow(img)

"""# # Crear un GridSearchCV y analizar la profundidad del árbol óptima"""

from sklearn.model_selection import GridSearchCV

# Define a list of possible values for the maximum depth of the decision tree
max_depths = range(1, 10)

# Define a dictionary of parameters to be searched
parameters = {'max_depth': max_depths}

# Create a GridSearchCV object
grid_search = GridSearchCV(tree_clf, parameters, cv=5)

# Fit the grid search object to the training data
grid_search.fit(X_train, y_train)

# Get the best model and its parameters
best_model = grid_search.best_estimator_
best_depth = best_model.max_depth

# Print the optimal depth of the decision tree
print(f"Mejor profundidad del árbol:", best_depth)
print(f"Mejor puntaje de validación: {grid_search.best_score_}")