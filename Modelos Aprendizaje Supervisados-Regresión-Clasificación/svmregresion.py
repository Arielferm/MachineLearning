# -*- coding: utf-8 -*-
"""SVMRegresion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UAtA9NkMdwIPYpMG9MVUaJgCwA92l7Ii
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

#Ahora las funciones utiles de sklearn para preprocesar datos y armar un pipeline
from sklearn.pipeline import Pipeline
import sklearn.preprocessing as pp

#Y la estrella de este notebook:
from sklearn import svm
from sklearn import datasets

import pandas as pd
from sklearn.model_selection import train_test_split

"""# SVM: Regresión

El equivalente a las clases ``LinearSVC`` y ``SVC`` para regresión son las clases ``LinearSVR`` y ``SVR``, con idénticos parámetros y kernels. A continuación mostraremos como se desempeñan en el dataset de Housing de california, que ya hemos usado antes.

Recordemos que el Clasificador intenta maximizar el margen entre clases, admitiendo o no excepciones segun el valor de ``C``. En el caso de Regresión, intenta meter todas los puntos dentro de un margen que nosotros especificamos con ``epsilon``, y admite excepciones segun el valor de ``C``.. Este  ``epsilon`` es un parámetro nuevo que el clasificador no tiene.

## Utilidades de Ploteo
"""

DEFAULT_AXES=[0, 1, 0, 1]

def plot_svm_regression(svm_reg, X, y, axes=DEFAULT_AXES):
    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)
    y_pred = svm_reg.predict(x1s)
    plt.plot(x1s, y_pred, "k-", linewidth=2, label=r"$\hat{y}$")
    plt.plot(x1s, y_pred + svm_reg.epsilon, "k--")
    plt.plot(x1s, y_pred - svm_reg.epsilon, "k--")
    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=5, facecolors='#FFAAAA')
    #plt.plot(X, y, "bo")
    plt.xlabel(r"$x_1$", fontsize=18)
    plt.legend(loc="upper left", fontsize=18)
    plt.axis(axes)

"""## Importamos / Generamos los datos"""

#Permito que este cuadernillo entre a mis archivos en Google Drive:
from google.colab import drive

#Por las dudas que ya se haya ejecutado este bloque borro todos los cambios:
drive.flush_and_unmount()

#el módulo drive nos permite acceder a nuestros archivos de Google Drive mediante la función mount:
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/ML/Student_Performance.csv")
data.head()

## Para las entradas Eliminamos la columna "Performance Index"
XX = data.drop(columns = "Performance Index")

## Como ya Analizamos los datos. Sabemos que la columna "Extracurricular Activities" no aporta información para la Performance. La Eliminamos
X = XX.drop(columns = "Extracurricular Activities")

## La salida es la columna "Performance Index". La separamos
y = data["Performance Index"]

"""### Veamos un poco más los datos!"""

plt.scatter(X["Previous Scores"],y, s = 1)
plt.show()

plt.scatter(X["Hours Studied"],y, s=1)
plt.show()

"""## Fiteamos con SVR

### Comenzaremos utilizando una sola entrada para poder verlo gráficamente

Abajo utilizaremos un kernel polinomial de grado 2. Colocando un ``epsilon=0.1``
"""

#Separamos el conjunto de testeo y de entrenamiento

X_r = X["Previous Scores"].values.reshape(10000,1)
y_r = y.values.reshape(10000,1)

scaler = pp.MinMaxScaler()
X_s = scaler.fit_transform(X_r)
y_s = scaler.fit_transform(y_r)


X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.4, random_state=42)

svm_poly_reg = svm.SVR(kernel="poly", degree=1, epsilon=0.005, gamma="scale")
svm_poly_reg.fit(X_train, y_train)

plot_svm_regression(svm_poly_reg, X_train, y_train)
plt.scatter(X_train,y_train, s = 1)
plt.show()

from sklearn.metrics import r2_score


y_pred = svm_poly_reg.predict(X_test)

print("r2 score:", r2_score(y_pred, y_test))
print("\n")

"""## Ejercicio 1: Modificar el epsilon a 0.05, 0.15 y 0.3 y analizar los resultados

Según los resultados, se observa que: épsilon define una zona de margen en la que los errores no se penalizan en el entrenamiento del modelo, asique, aumentar épsilon puede hacer el modelo más tolerante a pequeños errores, pero menos preciso. A demás, aumentar el grado (degree) puede acrecentar la complejidad del modelo y el riesgo de sobreajuste, y un valor más bajo como 1 resulta en un modelo lineal.

## Ejercicio 2: Modificar el grado y analizar los resultados
"""

x = range(y_test.shape[0])
plt.scatter(x, y_test, s = 3)
plt.scatter(x, y_pred, s = 3, facecolors='#FFAAAA')
plt.show()

# Veamos que pasa al variar ``epsilon``.

#Separamos el conjunto de testeo y de entrenamiento

X_r = X.values.reshape(10000,4)
y_r = y.values.reshape(10000,1)

scaler = pp.MinMaxScaler()
X_s = scaler.fit_transform(X_r)
y_s = scaler.fit_transform(y_r)

X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.4, random_state=1)

import seaborn as sns
from sklearn.metrics import r2_score


svm_poly_reg = svm.SVR(kernel="poly", degree=1, epsilon=0.1, gamma="scale")
svm_poly_reg.fit(X_train, y_train)

y_pred = svm_poly_reg.predict(X_test)

from sklearn import metrics
print("\n\n")
print("r2 score:", r2_score(y_test,y_pred))
print("\n\n")

x = range(y_test.shape[0])
plt.scatter(x, y_test, s = 3)
plt.scatter(x, y_pred, s = 3, facecolors='#FFAAAA')
plt.show()

"""## Ejercicio 3: Modificar el epsilon y el grado y analizar los resultados

Según los resultados, se observa que: al incrementar el grado, captura relaciones más complejas, pero puede sobre ajustar y disminuirlo, hace al modelo más simple y lineal, pero también puede sub ajustar. También, el epsilon al incrementar genera más tolerancia a pequeños errores, menos precisión y evita sobreajuste, y al disminuir, da menos tolerancia a errores, más precisión, riesgo de sobreajuste.
"""