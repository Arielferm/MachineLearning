# -*- coding: utf-8 -*-
"""Modelo predicción de abandono en apps de e-commerce e inversión.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ViFg1EmDi0CMdkAKMA_-liMlXygIoMEC

# Modelo de predicción de abandono en aplicaciones de comercio electrónico e inversión

Este modelo avanzado de aprendizaje automático permite predecir la pérdida de clientes, identificando aquellos de alto riesgo. Esta herramienta ayuda a las empresas a tomar decisiones informadas sobre retención, optimizando esfuerzos estratégicos para mantener a los usuarios y maximizar los ingresos. Es especialmente útil en sectores como el comercio electrónico, la investigación de mercado financiero y los sistemas comerciales automatizados, donde la retención de clientes es clave para el éxito.

Nota: Este es un caso de uso simplificado. Las situaciones reales de investigación de mercados financieros o análisis de mercado para la detección de anomalías requieren ajustes adicionales y mayor integridad de los datos.
"""

import pandas as pd

customer_data = pd.DataFrame({
    'age': [
        25, 30, 40, 22, 35, 45, 28, 32, 38, 50, 27, 33, 42, 29, 36,
        48, 23, 31, 39, 44, 26, 34, 41, 47, 55, 21, 37, 43, 49, 52
    ],
    'location': [
        'NY', 'CA', 'TX', 'FL', 'WA', 'NY', 'CA', 'TX', 'FL', 'WA',
        'NY', 'CA', 'TX', 'FL', 'WA', 'NY', 'CA', 'TX', 'FL', 'WA',
        'NY', 'CA', 'TX', 'FL', 'WA', 'NY', 'CA', 'TX', 'FL', 'WA'
    ],
    'transactions': [
        10, 20, 5, 12, 18, 7, 25, 15, 8, 30, 11, 22, 6, 14, 19,
        9, 21, 16, 10, 23, 13, 24, 7, 26, 17, 10, 20, 5, 12, 18
    ],
    'churn': [
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1
    ]
})

def engineer_features(data):
    processed_data = data.copy()
    processed_data = pd.get_dummies(processed_data, columns=['location'], drop_first=True)
    return processed_data

customer_data_processed = engineer_features(customer_data)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Separate features (X) and target (y)
X = customer_data_processed.drop('churn', axis=1)
y = customer_data_processed['churn']

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y # Added stratify to ensure a balanced distribution of 'churn' in test set
)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

"""Para ajustar el umbral de clasificación, primero obtenemos las probabilidades de predicción del modelo. Luego, podemos aplicar un umbral personalizado a estas probabilidades para generar nuevas predicciones binarias y evaluar su impacto en las métricas."""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Get prediction probabilities for the positive class (churn=1)
probabilities = model.predict_proba(X_test)[:, 1]

# Define a custom threshold (e.g., 0.3, 0.5, 0.7)
# A lower threshold increases recall (more churn predicted) but might decrease precision (more false positives)
# A higher threshold increases precision (fewer false positives) but might decrease recall (more false negatives)
custom_threshold = 0.5

# Apply the custom threshold to get new binary predictions
predictions_thresholded = (probabilities >= custom_threshold).astype(int)

print(f"\nEvaluación con un umbral de {custom_threshold:.2f}:")

# Calculate evaluation metrics with the new threshold
accuracy_th = accuracy_score(y_test, predictions_thresholded)
precision_th = precision_score(y_test, predictions_thresholded)
recall_th = recall_score(y_test, predictions_thresholded)
f1_th = f1_score(y_test, predictions_thresholded)

print(f"Accuracy: {accuracy_th:.4f}")
print(f"Precision: {precision_th:.4f}")
print(f"Recall: {recall_th:.4f}")
print(f"F1-Score: {f1_th:.4f}")

# Display the confusion matrix for the new threshold
print("\nConfusion Matrix (con umbral ajustado):")
conf_matrix_th = confusion_matrix(y_test, predictions_thresholded)
print(conf_matrix_th)

"""Se cambia el valor de `custom_threshold` en el código anterior y se vuelve a ejecutar la celda para observar cómo las métricas de precisión, recall y F1-score varían. Esto permite encontrar un equilibrio adecuado para el caso de uso específico, donde el costo de los falsos positivos y los falsos negativos puede ser diferente."""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# Optionally, display the confusion matrix for a more detailed view
print("\nConfusion Matrix:")
conf_matrix = confusion_matrix(y_test, predictions)
print(conf_matrix)

"""## Visualizar Matriz de Confusión

**Reasoning**:
To visualize the confusion matrix as a heatmap, I need to import the necessary libraries, `seaborn` and `matplotlib.pyplot`, and then use `seaborn.heatmap()` to create the visualization with appropriate labels and title.
"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.heatmap(
    conf_matrix_th,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Predicted Non-Churn', 'Predicted Churn'],
    yticklabels=['Actual Non-Churn', 'Actual Churn']
)
plt.title('Confusion Matrix with Adjusted Threshold')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""**Reasoning**:
Now that the confusion matrix heatmap has been generated, visualize the evaluation metrics (Accuracy, Precision, Recall, and F1-Score) from the `conf_matrix_th` which is the result of the adjusted threshold.


"""

metrics = {
    'Accuracy': accuracy_th,
    'Precision': precision_th,
    'Recall': recall_th,
    'F1-Score': f1_th
}

metrics_df = pd.DataFrame(metrics.items(), columns=['Metric', 'Score'])

plt.figure(figsize=(8, 5))
sns.barplot(x='Metric', y='Score', hue='Metric', data=metrics_df, palette='viridis', legend=False)
plt.title('Performance Metrics with Adjusted Threshold')
plt.ylim(0, 1) # Ensure y-axis is between 0 and 1 for scores
plt.ylabel('Score')
plt.xlabel('Metric')
plt.show()

"""### Explicación de las Visualizaciones y Métricas

**1. Matriz de Confusión (Heatmap):**

Esta gráfica nos ayuda a entender qué tan bien nuestro modelo predice la 'rotación de clientes' (churn). Es una tabla que compara las predicciones del modelo con la realidad:

*   **Verdaderos Negativos (True Negatives - TN):** Clientes que el modelo predijo correctamente que *no* rotarían (no se irían) y que en realidad *no* rotaron. (Celda superior izquierda)
*   **Falsos Positivos (False Positives - FP):** Clientes que el modelo predijo erróneamente que *sí* rotarían (se irían), pero que en realidad *no* lo hicieron. Esto puede significar que gastamos recursos en retener a clientes que no lo necesitaban. (Celda superior derecha)
*   **Falsos Negativos (False Negatives - FN):** Clientes que el modelo predijo erróneamente que *no* rotarían (no se irían), pero que en realidad *sí* lo hicieron. Esto es crítico porque perdimos la oportunidad de retener a un cliente valioso. (Celda inferior izquierda)
*   **Verdaderos Positivos (True Positives - TP):** Clientes que el modelo predijo correctamente que *sí* rotarían (se irían) y que en realidad *sí* lo hicieron. Estos son los clientes que podemos intentar retener proactivamente. (Celda inferior derecha)

Un buen modelo tendrá valores altos en Verdaderos Positivos y Verdaderos Negativos, y valores bajos en Falsos Positivos y Falsos Negativos.

**2. Métricas de Rendimiento (Gráfico de Barras):**

Este gráfico resume el desempeño del modelo con números fáciles de entender:

*   **Accuracy (Precisión General):** Representa la proporción de predicciones correctas sobre el total de predicciones. Un 80% de Accuracy significa que el modelo acertó en 8 de cada 10 predicciones. Es útil, pero a veces engañoso si hay un gran desequilibrio entre las clases (por ejemplo, muchos más clientes que no rotan que los que sí).
*   **Precision (Precisión):** Se enfoca en las predicciones positivas (clientes que se van). Nos dice, de todos los clientes que el modelo *predijo* que rotarían, ¿cuántos *realmente* rotaron? Una alta precisión significa menos 'alertas falsas' sobre la rotación.
*   **Recall (Sensibilidad o Cobertura):** Se enfoca en los clientes que *realmente* rotaron. Nos dice, de todos los clientes que *realmente* rotaron, ¿a cuántos el modelo logró identificar? Un alto recall es crucial para no perder a ningún cliente que está en riesgo de irse.
*   **F1-Score:** Es un promedio ponderado de Precision y Recall. Es útil cuando queremos un equilibrio entre identificar correctamente a los clientes que rotan (Recall) y no generar demasiadas alarmas falsas (Precision). Un F1-Score alto indica un buen equilibrio.
"""